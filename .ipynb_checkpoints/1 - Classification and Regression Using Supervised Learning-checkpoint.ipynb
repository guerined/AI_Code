{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common ways to impart artificial intelligence into a machine is through machine learning. The world of machine learning is broadly divided into supervised and unsupervised learning.\n",
    "\n",
    "Supervised learning refers to the process of building a machine learning model that is based on labeled training data.\n",
    "\n",
    "Unsupervised learning refers to the process of building a machine learning model without relying on labeled training data. Since there are no labels available, you need to extract insights based on just the data given to you. The tricky thing here is that we don't know exactly what the criteria of separation should be. Hence, an unsupervised learning algorithm needs to separate the given dataset into a number of groups in the best way possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "The process of classifaction is on such technique where we classify data into a given number of classes.\n",
    "\n",
    "In Machine Learning, classification solves the problem of identifying the category to which a new data point belongs. We build the classification model based on the training dataset containing data points and the corresponding labels.\n",
    "\n",
    "A good classification system makes it easy to find and retrieve data. This is used extensively in face recognition, spam identification, recommendation engines, and so on. The algorithms for data classification will come up with the right criteria to separate the given data into the given number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to provide a sufficiently large number of samples so that it can generalize those criteria. If there is an insufficient number of samples, then the algorithm will overfit to the training data. This means that it won't perform well on unknown data because it fine-tuned the model too much to fit into the patterns observed in training data. This is actually a very common problem that occurs in the world of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "Machine learning algorithms expect data to be formatted in a certain way before they start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([[5.1, -2.9, 3.3],\n",
    "                      [-1.2, 7.8, -6.1],\n",
    "                      [3.9, 0.4, 2.1],\n",
    "                      [7.3,-9.9,-4.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several different preprocessing techniques:\n",
    " - Binarization\n",
    " - Mean Removal\n",
    " - Scaling\n",
    " - Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization\n",
    "This process is used when we want to convert our numerical values into boolean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binarized data:\n",
      " [[1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Binarize data\n",
    "data_binarized = preprocessing.Binarizer(threshold=2.1).transform(input_data)\n",
    "print(\"\\nBinarized data:\\n\", data_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Removal\n",
    "Removing the mean is a common preprocessing technique used in machine learning. It is usefull to remove the mean from our deature vector, so that each feature is centered on zero. We do this in order to remove bias from the features in our feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEFORE:\n",
      "Mean = [ 3.775 -1.15  -1.3  ]\n",
      "Std deviation = [3.12039661 6.36651396 4.0620192 ]\n"
     ]
    }
   ],
   "source": [
    "#Print means and standard deviation\n",
    "print(\"\\nBEFORE:\")\n",
    "print(\"Mean =\", input_data.mean(axis=0))\n",
    "print(\"Std deviation =\", input_data.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER:\n",
      "Mean = [1.11022302e-16 0.00000000e+00 2.77555756e-17]\n",
      "Std deviation= [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Remove mean\n",
    "data_scaled = preprocessing.scale(input_data)\n",
    "print(\"\\nAFTER:\")\n",
    "print(\"Mean =\", data_scaled.mean(axis=0))\n",
    "print(\"Std deviation=\", data_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "The value of each feature can vary between many random values. So it becomes important to scale those features that it is a level playing field for the machine learning algorithm to train on. We don't want any feature to be artificially large or small just beacuse of the nature of the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min max scaled data:\n",
      " [[0.74117647 0.39548023 1.        ]\n",
      " [0.         1.         0.        ]\n",
      " [0.6        0.5819209  0.87234043]\n",
      " [1.         0.         0.17021277]]\n"
     ]
    }
   ],
   "source": [
    "#Min Max scaling\n",
    "data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "data_scaled_minmax = data_scaler_minmax.fit_transform(input_data)\n",
    "print(\"\\nMin max scaled data:\\n\", data_scaled_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "We use the process of normalization to modify the values in the feature vector so that we can measure them on a common scale. Some of the most common forms of normalization aim to modify the values so that they sum up to 1. \n",
    "L1 Normalization, which refers to Least Absolute Deviations, works by making sure that the sum of absolute values is 1 in each row. \n",
    "L2 Normalization, which refers to Least Squares, works bu making sure that the sum of squares is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
